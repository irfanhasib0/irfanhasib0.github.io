<html>
<body>
<div>
<p>

    <b><I>Boosting auxiliary task guidance: a probabilistic approach</b></I><br>
&nbsp;Irfan Mohammad Al Hasib, Sumaiya Saima Sultana, Imrad Zulkar Nyeen, Muhammad Abdus Sabur<br>
Published in International Journal of Artificial Intelligence. 
<a href="https://ijai.iaescore.com/index.php/IJAI/article/view/21444"> Volume 12</a>&nbsp;
<a href="https://ijai.iaescore.com/index.php/IJAI/article/view/21444/13532"> Download PDF</a><br><br>
    &nbsp;&nbsp;<b><I>Abstract</b></I><br>

This work aims to introduce a novel approach for auxiliary task guidance (ATG). In this approach, our goal is to achieve effective guidance from a suitable auxiliary task by utilizing the uncertainty in calculated gradients for a mini-batch of samples. Our method calculates a probabilistic fitness factor of the auxiliary task gradient for each of the shared weights to guide the main task at every training step of mini-batch gradient descent. We have shown that this proposed factor incorporates task specific confidence of learning to manipulate ATG in an effective manner. For studying the potency of the method, monocular visual odometry (VO) has been chosen as an application. Substantial experiments have been done on the KITTI VO dataset for solving monocular VO with a simple convolutional neural network (CNN) architecture. Corresponding results show that our ATG method significantly boosts the performance of supervised learning for VO. It also out performs state-of-the-art (SOTA) auxiliary guided methods we applied for VO. The proposed method is able to achieve decent scores (in some cases competitive)compared to existing SOTA supervised monocular VO algorithms, while keeping an exceptionally low parameter space in supervised regime.<br><br>

Keywords<br>
    &nbsp;&nbsp;Auxiliary task guidance; Computer vision; Deep learning; Multi task learning; Visual odometry;
</div>
</p>
    
<p>

    <b><I>Grad Queue : A probabilistic framework to reinforce sparse gradients</b></I><br>
&nbsp;Irfan Mohammad Al Hasib<br>
Published in arxiv for the time being. I have plan to publish it on a Journal in near future. 
<a href="https://arxiv.org/abs/2404.16917"> arxiv</a>&nbsp;
<a href="https://arxiv.org/pdf/2404.16917"> Download PDF</a><br><br>
    &nbsp;&nbsp;<b><I>Abstract</b></I><br>

Informative gradients are often lost in large batch updates. We propose a robust mechanism
to reinforce the sparse components within a random batch of data points. A finite queue of online
gradients is used to determine their expected instantaneous statistics. We propose a function to
measure the scarcity of incoming gradients using these statistics and establish the theoretical ground
of this mechanism. To minimize conflicting components within large mini-batches, samples are
grouped with aligned objectives by clustering based on inherent feature space. Sparsity is measured
for each centroid and weighted accordingly. A strong intuitive criterion to squeeze out redundant
information from each cluster is the backbone of the system. It makes rare information indifferent
to aggressive momentum also exhibits superior performance with larger mini-batch horizon. The
effective length of the queue kept variable to follow the local loss pattern. The contribution of
our method is to restore intra-mini-batch diversity at the same time widening the optimal batch
boundary. Both of these collectively drive it deeper towards the minima. Our method has shown
superior performance for CIFAR10, MNIST, and Reuters News category dataset compared to minibatch gradient descent.<br><br>

Keywords<br>
    &nbsp;&nbsp;Sparse Gradients; Optimization;
</div>
</p>
</body>
</html>